---
output:
  html_document: default
  pdf_document: default
---
```{r setup}

knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(RColorBrewer)
library(rstan)
library(bayesplot)
library(modelr)
library(tidybayes)
library(ggstance)
library(brms)
library(loo)
library(DescTools)
library(lme4)
library(gamlss)
library(dplyr)
library(progress)
library(ggplot2)
library(cowplot)

theme_set(theme_gray())

rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
devAskNewPage(ask = FALSE)

source("model_check.R")
```

In this document, we run the rational agent analysis for transit decision task and its corresponding visualizations in Fernandes et al. We will first use a statistical model to predict behavioral agents' response distributions to different trials given different assigned visualizations and arrival time distributions. 

The framework can be thought of as a function that estimates four parameters under the experimental design of Fernandes et al. (including the realized sample size):

1.  The *rational baseline* ($Rprior$): the performance of the rational agent without access to the visualization signals, i.e., with prior beliefs only.
2.  The *rational benchmark* ($R$): the performance of the rational agent with access to the signal (provided by the visualization), i.e., with posterior beliefs.
3.  The *behavioral score* ($B$): the expected score of the behavioral agent predicted by a generative statistical model.
4.  The *calibrated behavioral score* ($C$): the score of a rational agent on information structure $\pi^B$.

## Generate Behavioral Response

### Load and clean data

We start by loading the experiment data of Fernandes et al. We remove duplicated data and responses for Scenario 4, which was not analyzed in the original paper. We then do data transformations.

```{r dataload}
df = read.csv("data/final_trials.csv") %>%
  as_tibble()

df = df[!duplicated(df), ]

max_trial = max(df$trial)

df = df %>%
  mutate(
    trial_normalized = ((trial - max_trial) / max_trial) + 0.5
  )

df = df %>%
  filter(scenario != "s4") %>%
  mutate(scenario = factor(scenario))
```

### Linear Model of Response

We start as simply as possible by just modeling the distribution of response.

Before we fit the model to our data, let's check that our priors seem reasonable. We'll use a weakly informative prior for the intercept parameter since we want the population-level centered intercept to be flexible. We set the expected value of the prior on the intercept equal to the mean value of the response that we sampled.

```{r checkdistribution}
df %>%
  ggplot() +
  geom_histogram(aes(response)) +
  facet_grid(. ~ scenario)

df %>% group_by(scenario) %>% summarise(response = mean(response))
```

We start modeling by specifying the simplest possible model regressing response on scenario id. 

```{r}
# get_prior(ddata = df, family = "gaussian", bf(response ~ scenario))
# starting as simple as possible: learn the distribution of response over scenario
prior.response <- brm(data = df, family = "gaussian",
                      bf(response ~ scenario),
                      prior = c(prior(normal(10, 1), class = Intercept),
                                prior(normal(1, 0.5), class = b),
                                prior(normal(0, 0.5), class = sigma)),
                      sample_prior = "only",
                      iter = 3000, warmup = 500, chains = 2, cores = 2)
```

Let's take a look at our prior predictive distribution compared with observed data.To evaluate the fit of these and subsequent models, we use a custom visual grammar we developed for model checking to compare the model's posterior predictive distribution with the distribution in data. The red lines represent prediction from model and the blue line is the distribution of the observed data.

```{r}
prior.response %>% 
  mcplot() +
  mc_distribution("predictive") +
  mc_uncertainty_representation(c("static")) +
  mc_comparative_layout("superposition") +
  mc_conditional_variables(row_vars = vars(scenario))
```

Now, let's fit the model to data.

```{r model}
m.response <- brm(data = df, family = "gaussian",
                   bf(response ~ scenario),
                   prior = c(prior(normal(10, 1), class = Intercept),
                             prior(normal(1, 0.5), class = b),
                             prior(normal(0, 0.5), class = sigma)),
                   iter = 3000, warmup = 500, chains = 2, cores = 2,
               file = "models/response_scenario_mdl")
```

Let's check our posterior predictive distribution.

```{r}
m.response %>% 
  mcplot() +
  mc_distribution("predictive") +
  mc_uncertainty_representation(c("static")) +
  mc_comparative_layout("superposition") +
  mc_conditional_variables(row_vars = vars(scenario))
```

Our model is not sensitive to visualization condition and trial number, so we expect to see mismatches here.

Now we gradually add predictors for the visualization condition and the normalized trial number into the model.

Before we fit the model to our data, let's check that our priors seem reasonable. Since we are now including more slope parameters for visualization condition and the normalized trial number in our model, we can dial down the width of our prior for sigma to avoid over-dispersion of predicted responses.

```{r}
# get_prior(data = df, family = "gaussian", bf(response ~ scenario + vis + trial_normalized))
# model with scenario, vis condition, and normalized trial number
prior.response_scenario_vis_trial <- brm(data = df, family = "gaussian",
                                         bf(response ~ scenario + vis + trial_normalized),
                                         prior = c(prior(normal(10, 1), class = Intercept),
                                                    prior(normal(1, 0.5), class = b),
                                                    prior(normal(0, 0.3), class = sigma)),
                                          sample_prior = "only",
                                         iter = 3000, warmup = 500, chains = 2, cores = 2)
```

Let's look at our prior predictive distribution.

```{r}
prior.response_scenario_vis_trial %>% 
  mcplot() +
  mc_distribution("predictive") +
  mc_uncertainty_representation(c("static")) +
  mc_comparative_layout("superposition") +
  mc_conditional_variables(row_vars = vars(vis), col_vars = vars(scenario))
```

Now, let's fit the model to data.

```{r model2}
m.response_scenario_vis_trial <- brm(data = df, family = "gaussian",
               bf(response ~ scenario + vis + trial_normalized),
               prior = c(prior(normal(10, 1), class = Intercept),
                         prior(normal(1, 0.5), class = b),
                         prior(normal(0, 0.3), class = sigma)),
               iter = 3000, warmup = 500, chains = 2, cores = 2,
               file = "models/response_scenario_vis_trial_mdl")
```

Let's check our posterior predictive distribution.

```{r}
m.response_scenario_vis_trial %>% 
  mcplot() +
  mc_distribution("predictive") +
  mc_uncertainty_representation(c("static")) +
  mc_comparative_layout("superposition") +
  mc_conditional_variables(row_vars = vars(vis), col_vars = vars(scenario))
```

These don't look great. We can see that the pattern in each visualization condition and scenario combination is more complex than our model predicts. We then add interactions between predictors to see whether our model can fit better.

### Add Interactions

We next add interactions between the scenario and visualization condition, to allow different visualization conditions to be affected by scenarios differently, as well as different effects of trial number depending on both visualization and scenario.

We use the same priors as we did for the previous model. Now, let's fit the model to our data.

```{r model3}
m.response_scenario_vis_trial_interaction <- brm(data = df, family = "gaussian",
                                  bf(response ~ scenario * vis + trial_normalized),
                                  prior = c(prior(normal(10, 1), class = Intercept),
                                            prior(normal(1, 0.5), class = b),
                                            prior(normal(0, 0.3), class = sigma)),
                                  iter = 3000, warmup = 500, chains = 2, cores = 2,
                                  file = "models/response_scenario_vis_trial_interaction_mdl")
```

Let's check our posterior predictive distribution.

```{r}
m.response_scenario_vis_trial_interaction %>%
  mcplot() +
  mc_distribution("predictive") +
  mc_uncertainty_representation(c("static")) +
  mc_comparative_layout("superposition") +
  mc_conditional_variables(row_vars = vars(vis), col_vars = vars(scenario))
```

We continue to add interaction with the normalized trial number.

```{r}
m.response_scenario_vis_trial_interaction_all <- brm(data = df, family = "gaussian",
                                              bf(response ~ scenario * vis * trial_normalized),
                                              prior = c(prior(normal(10, 1), class = Intercept),
                                                        prior(normal(1, 0.5), class = b),
                                                        prior(normal(0, 0.3), class = sigma)),
                                              iter = 3000, warmup = 500, chains = 2, cores = 2,
                                              file = "models/response_scenario_vis_trial_interaction_all_mdl")
```

Let's check our posterior predictive distribution.

```{r}
m.response_scenario_vis_trial_interaction_all %>%
  mcplot() +
  mc_distribution("predictive") +
  mc_uncertainty_representation(c("static")) +
  mc_comparative_layout("superposition") +
  mc_conditional_variables(row_vars = vars(vis), col_vars = vars(scenario))
```

This looks better, but it is still having trouble fitting the data. It may be that the model is not capturing the individual variability in response patterns. Next we'll add hierarchy to our model.

### Add Hierarchy for Intercepts

We next random intercepts to account for differences based on participant id.

```{r model4}
m.response_scenario_vis_trial_interaction_all_mix <- brm(data = df, family = "gaussian",
                                                      formula = bf(response ~ scenario * vis * trial_normalized + (1 | participant)),
                                                      prior = c(prior(normal(10, 1), class = Intercept),
                                                                prior(normal(1, 0.5), class = b),
                                                                prior(normal(0, 0.3), class = sigma)),
                                                      iter = 4000, warmup = 1000, chains = 2, cores = 2,
                                                      file = "models/response_scenario_vis_trial_interaction_all_mix_mdl")
```

Let's check our posterior predictive distribution.

```{r}
m.response_scenario_vis_trial_interaction_all_mix %>%
  mcplot() +
  mc_distribution("predictive") +
  mc_uncertainty_representation(c("static")) +
  mc_comparative_layout("superposition") +
  mc_conditional_variables(row_vars = vars(vis), col_vars = vars(scenario))
```

### Add Sigma

We next fit a submodel to predict the standard deviation $\sigma$, starting with visualization condition, scenario, and their interaction, then gradually adding random effects. We also add a random slope to allow for different learning effects by participant to both submodels.

```{r model5}
m.response_scenario_vis_trial_interaction_all_mix_sigma <- brm(data = df, family = "gaussian",
                                                  formula = bf(response ~ scenario * vis * trial_normalized + (1 | participant),
                                                     sigma ~ vis * scenario),
                                                  prior = c(prior(normal(10, 1), class = Intercept),
                                                            prior(normal(1, 0.5), class = b),
                                                            prior(normal(0, 0.3), class = b, dpar = sigma)),
                                                  iter = 10000, warmup = 4000, chains = 4, cores = 4,
                                                  file = "models/response_scenario_vis_trial_interaction_all_mix_sigma_mdl")
```

```{r}
m.response_scenario_vis_trial_interaction_all_mix_sigma %>%
  mcplot() +
  mc_distribution("predictive") +
  mc_uncertainty_representation(c("static")) +
  mc_comparative_layout("superposition") +
  mc_conditional_variables(row_vars = vars(vis), col_vars = vars(scenario))
```

```{r}
m.response_scenario_vis_trial_interaction_all_mix_sigma_trial <- brm(data = df, family = "gaussian",
                                                            formula = bf(response ~ scenario * vis * trial_normalized + (trial_normalized | participant),
                                                                         sigma ~ vis * scenario * trial_normalized),
                                                            prior = c(prior(normal(10, 1), class = Intercept),
                                                                      prior(normal(1, 0.5), class = b),
                                                                      prior(normal(0, 0.3), class = b, dpar = sigma)),
                                                            iter = 10000, warmup = 4000, chains = 4, cores = 4,
                                                            file = "models/response_scenario_vis_trial_interaction_all_mix_sigma_trial_mdl")
```

```{r}
m.response_scenario_vis_trial_interaction_all_mix_sigma_trial %>%
  mcplot() +
  mc_distribution("predictive") +
  mc_uncertainty_representation(c("static")) +
  mc_comparative_layout("superposition") +
  mc_conditional_variables(row_vars = vars(vis), col_vars = vars(scenario))
```

```{r}
m.response_scenario_vis_trial_interaction_all_mix_sigma_trial_participant <- brm(data = df, family = "gaussian",
                                                                                 formula = bf(response ~ scenario * vis * trial_normalized + (trial_normalized | participant),
                                                                                              sigma ~ vis * scenario * trial_normalized + (trial_normalized | participant)),
                                                                                 prior = c(prior(normal(10, 1), class = Intercept),
                                                                                           prior(normal(1, 0.5), class = b),
                                                                                            prior(normal(0, 0.3), class = b, dpar = sigma)),
                                                                                 iter = 10000, warmup = 4000, chains = 4, cores = 4,
                                                                                 file = "models/response_scenario_vis_trial_interaction_all_mix_sigma_trial_participant_mdl")
```

```{r}
m.response_scenario_vis_trial_interaction_all_mix_sigma_trial_participant %>%
  mcplot() +
  mc_distribution("predictive") +
  mc_uncertainty_representation(c("static")) +
  mc_comparative_layout("superposition") +
  mc_conditional_variables(row_vars = vars(vis), col_vars = vars(scenario))
```

### Final model

Next, we add two additional predictors to capture two critical parameters of the arrival time distributions that are used to generate the trial stimuli. We confirm via model checking that the model fits the distribution well. 

```{r modelfinal}
m.final_response <- brm(data = df, family = "gaussian",
                        formula = bf(response ~ scenario * vis * trial_normalized + mu + sigma + (trial_normalized | participant),
                                      sigma ~ vis * scenario * trial_normalized + (trial_normalized | participant)),
                        prior = c(prior(normal(10, 1), class = Intercept),
                                  prior(normal(1, 0.5), class = b),
                                  prior(normal(0, 0.3), class = b, dpar = sigma)),
                        iter = 10000, warmup = 4000, chains = 4, cores = 4,
                        file = "models/response_final_mdl")
```

```{r}
m.final_response %>%
  mcplot() +
  mc_distribution("predictive") +
  mc_uncertainty_representation(c("static")) +
  mc_comparative_layout("superposition") +
  mc_conditional_variables(row_vars = vars(vis), col_vars = vars(scenario))
```


### Generating behavioral agents' responses

We now simulate behavioral agents' responses using the posterior predictive distribution of the model, separately for each combination of trial parameters (including scenario, visualization condition, trial number, and stimuli distribution parameters). Because the response variable is discrete (numbers of minutes as integer) we calculate a discrete version of the distributions. We also generate the quantile intervals of $60\%$, $85\%$, and $99\%$ that are shown in the text conditions as we need these to simulate behavioral agent scores in those conditions.

```{r dataoutput}
distribution_df = df %>%
  dplyr::select(distribution, mu, sigma, nu, tau) %>%
  unique() %>%
  rowwise() %>%
  mutate(discrete = list(dBCT((-14:30) + 15, mu, sigma, nu, tau))) %>%
  unnest_wider(col = discrete, names_sep = '_') %>%
  mutate(
    text = round(gamlss.dist::qBCT(1 - .85, mu, sigma, nu, tau) - 15),
    text60 = round(gamlss.dist::qBCT(1 - .60, mu, sigma, nu, tau) - 15),
    text99 = round(gamlss.dist::qBCT(1 - .99, mu, sigma, nu, tau) - 15)
  )

meta.df = df %>% 
  group_by(scenario) %>% 
  data_grid(vis, trial_normalized, distribution) %>%
  merge(distribution_df %>% dplyr::select(distribution, mu, sigma, nu, tau), by="distribution")

pred.df = meta.df %>%
  add_predicted_draws(m.final_response, ndraws = 500, value="pred_response", re_formula = NA) %>%
  ungroup()
```

## Rational Agent Framework

### Scoring rule

First we define the scoring rules for each analyzed scenario in the transit decision problem. The rules allow us to calculate the expected payoff over a bus arriving distribution given a participant's stated arrival time at the stop.

```{r scoringrule}
scoring_rule = function(rider_at_stop, distribution, scenario) {
  all_scenario_params = list(
    # delay_bonus, wait_penalty, destination_reward, and max_destination_time
    s1 = c(8, 14, 14, 90),
    s2 = c(14, 14, 14, 60),
    s3 = c(8, 17, 17, 120)
  )
  scenario_params = all_scenario_params[[scenario]]
  
  payoff <- function(rider_at_stop, bus_at_stop, second_bus_at_stop, wait_penalty,
                     delay_bonus, destination_reward, max_destination_time){
  
          arrived_before_bus <- as.integer(rider_at_stop <= bus_at_stop)
          arrived_after_bus <- as.integer(rider_at_stop > bus_at_stop)
          time_waited <- (bus_at_stop * arrived_before_bus) +
                         (second_bus_at_stop * arrived_after_bus) - rider_at_stop
          time_at_destination <- max(max_destination_time -
                                # (bus_at_stop * arrived_before_bus) -
                                ((second_bus_at_stop - bus_at_stop) * arrived_after_bus), 0)
  
          reward <- (delay_bonus * rider_at_stop) -
                    (time_waited * wait_penalty) +
                    (time_at_destination * destination_reward)
          return(reward)
  }
  
  pointwise.payoff <- lapply(0:(45 * 45 - 1),
                             function(m,
                                      wait_penalty,
                                      delay_bonus,
                                      destination_reward,
                                      max_destination_time){
                               first_bus = (m %/% 45) - 14
                               second_bus = (m %% 45) - 14
                            first_prob <- distribution[first_bus + 15]
                            second_prob <- distribution[second_bus + 15]
                            payoff <- payoff(rider_at_stop, first_bus, second_bus + 30, wait_penalty,
                                             delay_bonus, destination_reward,
                                             max_destination_time)
                            return(payoff * first_prob * second_prob)
                      }, wait_penalty = scenario_params[2],
                         delay_bonus = scenario_params[1],
                         destination_reward =  scenario_params[3],
                         max_destination_time = scenario_params[4])

  Reduce('+', pointwise.payoff)
}
```

### Setup

We prepare by calculating and transforming some variables.

```{r precalculation}
scenario_ids = unique(df$scenario)
distribution_ids = unique(df$distribution)
vis_ids = unique(df$vis)

# round the response predicted by model into interger and then bound them in [1, 30]
pred.df$pred_response = round(pmax(pmin(pred.df$pred_response, 30), 1))
```

We create a dictionary of payoffs for all scenario, stimuli distributions, and rider arivial times at the stop to accelerate later calculations. This may take a few minutes.

```{r payoff mapping}
payoff_mapping = list()
for (scenario_id in scenario_ids) {
  distribution_seq = list()
  for (distribution_id in distribution_ids) {
    response_seq = sapply(0:30, 
                          function(response, distribution_id, scenario_id) {
                            scoring_rule(response,
                                         distribution_df %>%
                                            filter(distribution == distribution_id) %>%
                                            dplyr::select("discrete_1":"discrete_45") %>%
                                            as.vector() %>%
                                            as.numeric(),
                                         scenario_id)
                          }, 
                          distribution_id = distribution_id,
                          scenario_id = scenario_id)
    distribution_seq[[distribution_id]] = response_seq
  }
  payoff_mapping[[scenario_id]] = distribution_seq
}
```

We define some functions to calculate the four parameters of interest in the rational agent framework.

```{r setup rational agent framework}
get_rational_posterior = function(sample_df, distribution_df) {
  
  # for visualization with full information, calculate the optimal payoff in 0:30 response for each scenario and distribution
  posterior_optimal_payoff = list()
  for (scenario_id in scenario_ids) {
    distribution_seq = list()
    for (distribution_id in distribution_ids) {
      optimal_payoff = max(payoff_mapping[[scenario_id]][[distribution_id]])
      distribution_seq[[distribution_id]] = optimal_payoff
    }
    posterior_optimal_payoff[[scenario_id]] = distribution_seq
  }
  
  sample_with_distribution = sample_df %>% merge(distribution_df,
                                                   by="distribution")
  
  # for partial information (text showing 60% interval, 80% interval, 99% interval), 
  # calculate the optimal payoff for each scenario and the interval value
  vis_value_distribution_mapping = list()
  for (scenario_id in scenario_ids) {
    case_data = sample_with_distribution %>% filter(scenario == scenario_id)
    vis_names = c("text", "text60", "text99")
    vis_seq = list()
    for (vis_name in vis_names) {
      values = unique(case_data[[vis_name]])
      value_seq = list()
      for (value in values) {
        distributions = case_data[case_data[vis_name] == value, ] %>% dplyr::select("discrete_1":"discrete_45") %>% data.matrix()
        
        distribution = colSums(distributions) / nrow(distributions)
        payoff = Reduce("max", lapply(0:30, 
                                function(m, distribution, scenario) scoring_rule(m, distribution, scenario), 
                                distribution = distribution, 
                                scenario = scenario_id))
        value_seq[[as.character(value)]] = payoff
      }
      vis_seq[[vis_name]] = value_seq
    }
    vis_value_distribution_mapping[[scenario_id]] = vis_seq
  }
  
  # function used to retrieve optimal payoff from precalculated arrays
  get_payoff = function(scenario_id, vis, data) {
    scenario_id = as.character(scenario_id)
    if (!startsWith(vis, "text")) {
      return (posterior_optimal_payoff[[scenario_id]][[data[["distribution"]] ]])
    } else {
      return (vis_value_distribution_mapping[[scenario_id]][[vis]][[as.character(data[[vis]])]])
    }
  }
  
  # simulate rational agent for every line in experiment data
  posterior_action_payoff = c()
  for (i in 1:nrow(sample_with_distribution)) {
    new_payoff = get_payoff(sample_with_distribution[i, "scenario"],
                                           sample_with_distribution[i, "vis"],
                                           sample_with_distribution[i, ])
    posterior_action_payoff = c(posterior_action_payoff, 
                                new_payoff)
  }
  
  sample_with_distribution %>%
    mutate(posterior_payoff = posterior_action_payoff) %>%
    group_by(scenario) %>%
    summarise(payoff = mean(posterior_payoff))
}

get_rational_prior = function(sample_df, distribution_df) {
  prior_belief = sample_df %>% 
    merge(distribution_df, by="distribution")
  prior_action_payoff = data.frame(matrix(ncol=2,nrow=0, 
                                         dimnames=list(NULL, c("scenario", "payoff"))))
  for (scenario_id in scenario_ids) {
    # the distributions in the scenario
    distributions = prior_belief %>% 
      filter(scenario == scenario_id) %>% 
      dplyr::select("discrete_1":"discrete_45") %>% 
      data.matrix()
    
    # the expected distribution in the scenario
    distribution = colSums(distributions) / nrow(distributions)
    
    # optimal payoff of 0:30 response
    payoff = Reduce("max", lapply(0:30, 
                                  function(m, distribution, scenario) scoring_rule(m, distribution, scenario), 
                                  distribution = distribution, 
                                  scenario = scenario_id))
    prior_action_payoff[nrow(prior_action_payoff) + 1,] = c(scenario_id, payoff)
  }
  prior_action_payoff = prior_action_payoff %>% transform(payoff = as.numeric(payoff))
  prior_action_payoff
}

get_behavioral = function(sample_df, distribution_df) {
  # for behavioral agent's payoff, we retrieve the payoff for the scenario, the distribution, and the response in precalculated arrays
  sample_df %>%
    ungroup() %>%
    mutate(scenario = as.character(scenario)) %>% 
    rowwise() %>% 
    mutate(behavioral_payoff = payoff_mapping[[scenario]][[distribution]][[pred_response + 1]]) %>%
    group_by(scenario, vis) %>%
    summarise(payoff = mean(behavioral_payoff))
}

get_calibrated_payoff_mapping = function(sample_df, distribution_df) {
  # return the optimal payoff based on the distribution rational agent learns when viewing a joint distribution of response, scenario, visualization, and distribution
  sample_with_distribution = sample_df %>% merge(distribution_df,
                                                   by="distribution")
  calibrated_payoff_mapping = list()
  for (scenario_id in scenario_ids) {
    scenario_list = list()
    case_data = sample_with_distribution %>% filter(scenario == scenario_id)
    responses = unique(case_data$pred_response)
    for (response_id in responses) {
      # the distributions in the scenario, the visualization condition, and the behavioral response
      distributions = case_data %>% 
        filter(pred_response == response_id) %>% 
        dplyr::select("discrete_1":"discrete_45") %>% 
        data.matrix()
      # the expected distribution
      distribution = colSums(distributions) / nrow(distributions)
      calibrated_response = which.max(lapply(0:30, 
                      function(m, distribution, scenario) scoring_rule(m, distribution, scenario),
                      distribution = distribution,
                      scenario = scenario_id))
      
      scenario_list[[as.character(response_id)]] = calibrated_response
    }
    calibrated_payoff_mapping[[scenario_id]] = scenario_list
  }
  calibrated_payoff_mapping
}

get_calibrated_behavioral = function(sample_df, calibrated_payoff_mapping) {
  sample_df %>%
    ungroup() %>%
    mutate(scenario = as.character(scenario)) %>% 
    rowwise() %>% 
    mutate(calibrated_payoff = payoff_mapping[[scenario]][[distribution]][[calibrated_payoff_mapping[[scenario]][[as.character(pred_response)]]]]) %>%
    group_by(scenario, vis) %>%
    summarise(payoff = mean(calibrated_payoff))
  
  # calibrated_behavioral_payoff = tibble(
  #   "scenario" = sample_df$scenario,
  #   "vis" = sample_df$vis,
  #   "response" = sample_df$pred_response,
  #   "payoff" = -1
  # )
  # sample_with_distribution = sample_df %>% merge(distribution_df,
  #                                                  by="distribution")
  # for (scenario_id in scenario_ids) {
  #   for (vis_id in vis_ids) {
  #     case_data = sample_with_distribution %>% filter(scenario == scenario_id,
  #                                                    vis == vis_id)
  #     responses = unique(case_data$pred_response)
  #     for (response_id in responses) {
  #       # the distributions in the scenario, the visualization condition, and the behavioral response
  #       distributions = case_data %>% 
  #         filter(pred_response == response_id) %>% 
  #         dplyr::select("discrete_1":"discrete_45") %>% 
  #         data.matrix()
  #       # the expected distribution
  #       distribution = colSums(distributions) / nrow(distributions)
  #       # the optimal payoff in the expected distribution
  #       payoff = Reduce("max", lapply(0:30, 
  #                       function(m, distribution, scenario) scoring_rule(m, distribution, scenario), 
  #                       distribution = distribution, 
  #                       scenario = scenario_id))
  #       calibrated_behavioral_payoff[calibrated_behavioral_payoff$scenario == scenario_id &
  #                                      calibrated_behavioral_payoff$vis == vis_id &
  #                                      calibrated_behavioral_payoff$response == response_id, "payoff"] = payoff
  #     }
  #   }
  # }
  # calibrated_behavioral_payoff %>%
  #   group_by(scenario, vis) %>%
  #   summarise(payoff = mean(payoff))
}

```

### Pre-experiment analysis

We calculate the rational baseline and benchmark. This may take a few minutes.

```{r calculation rational posterior}
all_rational_posterior = get_rational_posterior(meta.df, distribution_df)
all_rational_posterior
```

```{r calculation rational prior}
all_rational_prior = get_rational_prior(meta.df, distribution_df)
all_rational_prior
```

### Post-experiment analysis

We calculate the behavioral agents' expected score and the expected calibrated behavioral score. To capture uncertainty, we simulate the experiment a hundred times, drawing a new sample of participant arrival time decisions in each round. This provides us with a distribution comprised of 100 simulated scores This will take a few hours. 

```{r calculation behavioral}
n_round = 500
sample_size = 10

all_behavioral = tibble()
all_calibrated_behavioral = tibble()

# s1_response_id = sort(unique((pred.df |> filter(scenario == "s1"))$pred_response))
# s2_response_id = sort(unique((pred.df |> filter(scenario == "s2"))$pred_response))
# s3_response_id = sort(unique((pred.df |> filter(scenario == "s3"))$pred_response))
# 
# while (TRUE) {
#   sample_df = pred.df %>% group_by(.row) %>% sample_n(100)
#   if (identical(sort(unique((sample_df |> filter(scenario == "s1"))$pred_response)), s1_response_id) &&
#       identical(sort(unique((sample_df |> filter(scenario == "s2"))$pred_response)), s2_response_id) &&
#       identical(sort(unique((sample_df |> filter(scenario == "s3"))$pred_response)), s3_response_id)) {
#     break
#   }
#   print(sort(unique((sample_df |> filter(scenario == "s1"))$pred_response)))
#   print(sort(unique((sample_df |> filter(scenario == "s2"))$pred_response)))
#   print(sort(unique((sample_df |> filter(scenario == "s3"))$pred_response)))
# }

sample_df = rbind(pred.df %>% group_by(scenario, pred_response) %>% slice(1),
                  pred.df %>% group_by(.row) %>% sample_n(sample_size * 10))

calibrated_payoff_mapping = get_calibrated_payoff_mapping(sample_df, distribution_df)

pb <- progress_bar$new(
  format = "  behavioral [:bar] :percent eta: :eta",
  total = n_round, clear = FALSE, width= 60)
# we run n_round rounds to generate n_round samples and calculate behavioral agent's score and calibrated score for each sample
for (round_id in 1:n_round) {
  pb$tick()
  # sample
  sample_df = pred.df %>% group_by(.row) %>% sample_n(sample_size)
  
  #behavioral score
  behavioral = get_behavioral(sample_df, distribution_df)
  behavioral$round = round_id
  all_behavioral = rbind(all_behavioral, behavioral)
  
  # calibrated score
  calibrated_behavioral = get_calibrated_behavioral(sample_df, calibrated_payoff_mapping)
  calibrated_behavioral$round = round_id
  all_calibrated_behavioral = rbind(all_calibrated_behavioral, calibrated_behavioral)
}
```

### Results

We show the results by visualizations in each scenario.

```{r visualization}
score_summary = merge(all_behavioral %>% 
        dplyr::select(behavioral_score = payoff, everything()) %>% 
        group_by(scenario, vis) %>% 
        dplyr::summarise(behavioral_score = mean(behavioral_score)), 
  all_calibrated_behavioral %>% 
    dplyr::select(calibrated_score = payoff, everything()) %>% 
    group_by(scenario, vis) %>% 
    dplyr::summarise(calibrated_score = mean(calibrated_score)), by = c("scenario", "vis")) %>%
  merge(all_rational_posterior %>% 
          dplyr::select(rational_posterior_score = payoff, everything()), by = c("scenario")) %>%
  merge(all_rational_prior %>% 
          dplyr::select(rational_prior_score = payoff, everything()), by = c("scenario")) %>%
  mutate(decision_loss = calibrated_score - behavioral_score,
         differentiation_loss = rational_posterior_score - calibrated_score,
         delta = rational_posterior_score - rational_prior_score) %>%
  arrange(decision_loss) %>%
  mutate(r1 = round(differentiation_loss / delta * 100), 
         r2 = round(decision_loss / delta * 100))
```

```{r s1}
ggplot() +
  stat_slab(data = all_behavioral %>% filter(scenario == "s1"), aes(y = vis, x = payoff), fill = "#7570b3", point_size=3) +
  stat_slab(data = all_calibrated_behavioral %>% filter(scenario == "s1"), aes(y = vis, x = payoff), fill = "#d95f02", point_size=3) +
  geom_vline(xintercept = (all_rational_posterior %>% filter(scenario == "s1"))$payoff, linetype = "dashed", linewidth = 1) + 
  geom_vline(xintercept = (all_rational_prior %>% filter(scenario == "s1"))$payoff, linetype = "dashed", linewidth = 1) + 
  labs(x = "", y = "") +
  ylim((score_summary %>% filter(scenario == "s1"))$vis) +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        panel.grid.major = element_line(colour = "grey"),
        axis.line.x = element_line(linewidth = 1.5, colour = "grey80"),
        panel.background = element_rect(fill = "white", color = "white"),
        axis.ticks.y = element_blank(),
        axis.ticks.x = element_line(colour = "grey"))
```

```{r s2}
ggplot() +
  stat_slab(data = all_behavioral %>% filter(scenario == "s2"), aes(y = vis, x = payoff), fill = "#7570b3", point_size=3) +
  stat_slab(data = all_calibrated_behavioral %>% filter(scenario == "s2"), aes(y = vis, x = payoff), fill = "#d95f02", point_size=3) +
  geom_vline(xintercept = (all_rational_posterior %>% filter(scenario == "s2"))$payoff, linetype = "dashed", size = 1) + 
  geom_vline(xintercept = (all_rational_prior %>% filter(scenario == "s2"))$payoff, linetype = "dashed", size = 1) + 
  labs(x = "", y = "") +
  ylim((score_summary %>% filter(scenario == "s2"))$vis) +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        panel.grid.major = element_line(colour = "grey"),
        axis.line.x = element_line(linewidth = 1.5, colour = "grey80"),
        panel.background = element_rect(fill = "white", color = "white"),
        axis.ticks.y = element_blank(),
        axis.ticks.x = element_line(colour = "grey"))
```

```{r s3}
ggplot() +
  stat_slab(data = all_behavioral %>% filter(scenario == "s3"), aes(y = vis, x = payoff), fill = "#7570b3", point_size=3) +
  stat_slab(data = all_calibrated_behavioral %>% filter(scenario == "s3"), aes(y = vis, x = payoff), fill = "#d95f02", point_size=3) +
  geom_vline(xintercept = (all_rational_posterior %>% filter(scenario == "s3"))$payoff, linetype = "dashed", linewidth = 1) + 
  geom_vline(xintercept = (all_rational_prior %>% filter(scenario == "s3"))$payoff, linetype = "dashed", linewidth = 1) + 
  labs(x = "", y = "") +
  ylim((score_summary %>% filter(scenario == "s3"))$vis) +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        panel.grid.major = element_line(colour = "grey"),
        axis.line.x = element_line(linewidth = 1.5, colour = "grey80"),
        panel.background = element_rect(fill = "white", color = "white"),
        axis.ticks.y = element_blank(),
        axis.ticks.x = element_line(colour = "grey"))
```
