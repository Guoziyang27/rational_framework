'''
In this document, we use the data generated by `data_generation.Rmd` and run the rational framework.
This file takes an input of `./fernandes_data.csv` and outputs four `csv` files of the scores of 
rational agent with posterior information, rational agent with prior information, behavioral agent, 
and calibrated behavioral agent. 
'''

import os

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from pandas import read_csv
from tqdm import tqdm

def scoring_rule(action, distribution, scenario):

    '''
    The scoring rule of three scenarios, which is from Fernandes et al. 
    - Input
        - action: the evaluted arrival time.
        - distribution: the distribution of bus arrival time, which is discreted into [1, 30].
        - scenario: the scenario that action is evaluated in (s1: Brunch With Friends; s2: Sunday Festival; 
        s3: Sunday Museum)
    - Output
        - the payoff
    '''

    all_scenario_params = {
        # delay_bonus, wait_penalty, destination_reward, max_destination_time
        "s1": [8, -14, 14, 90],
        "s2": [14, -14, 14, 60],
        "s3": [8, -17, 17, 120]
    }
    scenario_params = all_scenario_params[scenario]

    return scenario_params[0] * action + \
        np.sum([[(scenario_params[1] * ((t2 + 1 + 30 if t1 < action else t1 + 1) - action) +
                  max(scenario_params[2] * (scenario_params[3] - (t2 - t1 + 30 if t1 < action else 0)), 0)) *
                 distribution[t1] * distribution[t2]
                 for t2 in range(30)]
                for t1 in range(30)])


def get_rational_posterior(sample_data, distribution_info):
    '''
    This function returns the score of rational agent with posterior information based on `sample_data` and 
    `distribution_info`.
    - Input
        - sample_data: the data set which rational agent is learning on.
        - distribution_info: the dataframe for distribution as discrete numbers.
    - Output
        - the upperbound of payoff for each scenario.
    '''
    posterior_action_payoff_mapping = np.max(action_payoff_mapping, axis=2)

    sample_data_with_distribution = sample_data.join(distribution_info.set_index("distribution"),
                                                     on="distribution")

    def get_rational_posterior_score(scenario, vis, data):
        return posterior_action_payoff_mapping[
                  int(scenario[1:]) - 1][
                  np.where(distribution_ids == data["distribution"])[0][0]]

    posterior_action_payoff = pd.DataFrame({
        "scenario": sample_data_with_distribution["scenario"],
        "vis": sample_data_with_distribution["vis"],
        "score": [get_rational_posterior_score(sample_data_with_distribution.loc[i, "scenario"],
                                               sample_data_with_distribution.loc[i, "vis"],
                                               sample_data_with_distribution.loc[i, :])
                  for i in range(len(sample_data_with_distribution))]
    })
    return posterior_action_payoff.groupby(["scenario"]).agg({"score": "mean"}).reset_index()


def get_rational_prior(sample_data, distribution_info):
    '''
    This function returns the score of rational agent with prior information based on `sample_data` and 
    `distribution_info`.
    - Input
        - sample_data: the data set which rational agent is learning on.
        - distribution_info: the dataframe for distribution as discrete numbers.
    - Output
        - the payoff of rational agent with prior information for each scenario.
    '''
    prior_belief = sample_data.groupby(["scenario", "distribution"]).count().reset_index().loc[:,
                   ["scenario", "distribution", ".row"]]

    num_scenario = len(np.unique(prior_belief["scenario"].values))

    prior_belief = prior_belief.join(distribution_info.set_index("distribution"),
                                     on="distribution")

    distribution = prior_belief.loc[:, "discrete_1":"discrete_30"].values.reshape([num_scenario, -1, 30])

    count_distribution = prior_belief.loc[:, ".row"].values.reshape([num_scenario, -1])
    prior_distribution = np.sum(distribution * np.repeat(count_distribution, 30, axis=1).reshape([3, 30, 30]), axis=1)
    prior_distribution /= np.sum(count_distribution, axis=1, keepdims=True)

    prior_action_payoff = np.array([[scoring_rule(action, prior_distribution[scenario], "s" + str(scenario + 1))
                                      for action in range(1, 31)]
                                     for scenario in range(num_scenario)])
    optimal_prior_payoff = np.max(prior_action_payoff, axis=1)
    return pd.DataFrame({
        "scenario": ["s1", "s2", "s3"],
        "score": optimal_prior_payoff
    })


def get_behavioral(sample_data, distribution_info):
    '''
    This function returns the score of behavioral agent based on `sample_data` and `distribution_info`.
    - Input
        - sample_data: the data set which rational agent is learning on.
        - distribution_info: the dataframe for distribution as discrete numbers.
    - Output
        - the payoff of behavioral for each scenario and visualization.
    '''

    behavioral_payoff = pd.DataFrame({
        "scenario": sample_data["scenario"],
        "vis": sample_data["vis"],
        "score": [action_payoff_mapping[
                      int(sample_data.loc[i, "scenario"][1:]) - 1][
                      np.where(distribution_ids == sample_data.loc[i, "distribution"])[0][0]][
                      int(sample_data.loc[i, "pred_response"]) - 1]
                  for i in range(len(sample_data))]
    })
    return behavioral_payoff.groupby(["scenario", "vis"]).agg({"score": "mean"}).reset_index()


def get_calibrated_behavioral(sample_data, distribution_info):
    '''
    This function returns the score of behavioral agent calibrated by rational agent based on `sample_data` 
    and `distribution_info`.
    - Input
        - sample_data: the data set which rational agent is learning on.
        - distribution_info: the dataframe for distribution as discrete numbers.
    - Output
        - the payoff of calibrated behavioral for each scenario and visualization.
    '''
    belief_after_behavioral = sample_data.groupby(["scenario", "vis", "pred_response", "distribution"])\
                                  .count().reset_index()\
                                  .loc[:, ["scenario", "vis", "pred_response", "distribution", ".row"]] \
                                  .join(distribution_info.set_index("distribution"),
                                        on="distribution")
    calibrated_behavioral_payoff = pd.DataFrame({
        "scenario": sample_data["scenario"],
        "vis": sample_data["vis"],
        "response": sample_data["pred_response"],
        "score": np.nan
    })
    for scenario in scenario_ids:
        for vis in vis_ids:
            case_data = belief_after_behavioral.loc[(belief_after_behavioral["scenario"] == scenario) &
                                                    (belief_after_behavioral["vis"] == vis), :]
            responses = np.unique(case_data["pred_response"].values)
            for response in responses:
                distributions = case_data.loc[case_data["pred_response"] == response, "discrete_1":"discrete_30"].values
                counts = case_data.loc[case_data["pred_response"] == response, ".row"].values
                distributions *= np.repeat(counts, 30, axis=0).reshape([counts.shape[0], -1]) / np.sum(counts)
                distribution = np.sum(distributions, axis=0)
                optimal_payoff = np.max([scoring_rule(action, distribution, scenario) for action in range(1, 31)])
                calibrated_behavioral_payoff.loc[(calibrated_behavioral_payoff["scenario"] == scenario) &
                                                 (calibrated_behavioral_payoff["vis"] == vis) &
                                                 (calibrated_behavioral_payoff["response"] == response), "score"] = \
                    optimal_payoff
    return calibrated_behavioral_payoff.groupby(["scenario", "vis"]).agg({"score": "mean"}).reset_index()


if __name__ == "__main__":
    # load data
    data = read_csv("./data/fernandes_data.csv")
    distribution_info = read_csv("./data/distribution_info_bct.csv")

    scenario_ids = np.unique(data["scenario"].values)
    distribution_ids = np.unique(data["distribution"].values)
    vis_ids = np.unique(data["vis"].values)

    # round and clip pred_response into discrete number in [1, 30]
    data["pred_response"] = np.clip(np.round(data["pred_response"]), 1, 30)

    # pre-calculate payoff matrix to fasten later calculation
    action_payoff_mapping = [[[scoring_rule(action,
                                            distribution_info.loc[
                                                distribution_info["distribution"] == distribution_id,
                                                "discrete_1":"discrete_30"].values[0],
                                            scenario)
                               for action in range(1, 31)]
                              for distribution_id in distribution_ids.tolist()]
                             for scenario in scenario_ids.tolist()]

    # calculate rational agent's scores
    meta_data = data.loc[:, data.columns != 'pred_response'].drop_duplicates().reset_index(drop=True)

    print("Calculating rational posterior scores...", end="")
    all_rational_posterior = get_rational_posterior(meta_data, distribution_info)
    print("[DONE]")

    all_rational_posterior.to_csv("./data/all_rational_posterior.csv")

    print("Calculating rational prior scores...", end="")
    all_rational_prior = get_rational_prior(meta_data, distribution_info)
    print("[DONE]")

    all_rational_prior.to_csv("./data/all_rational_prior.csv")

    # calculate behavioral agent's scores

    n_round = 1000
    n_sample = 10

    all_behavioral = pd.DataFrame()
    all_calibrated_behavioral = pd.DataFrame()

    t = tqdm(range(n_round))
    for round_id in t:
        sample_data = data.groupby([".row"]).sample(n_sample).reset_index()

        t.set_description("Rand %i (Behavioral)" % (round_id + 1))
        t.refresh()
        one_behavioral = get_behavioral(sample_data, distribution_info)
        one_behavioral["round"] = round_id
        all_behavioral = pd.concat([all_behavioral, one_behavioral])

        t.set_description("Rand %i (Calibrated Behavioral)" % (round_id + 1))
        t.refresh()
        one_calibrated_behavioral = get_calibrated_behavioral(sample_data, distribution_info)
        one_calibrated_behavioral["round"] = round_id
        all_calibrated_behavioral = pd.concat([all_calibrated_behavioral, one_calibrated_behavioral])

    all_behavioral.to_csv("./data/all_behavioral.csv")
    all_calibrated_behavioral.to_csv("./data/all_calibrated_behavioral.csv")
