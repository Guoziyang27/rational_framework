\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage{algorithm2e}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}

\SetKwComment{Comment}{/* }{ */}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\input{notation}

\title{Guideline for rational agent framework}

\usepackage{Sweave}
\begin{document}

\maketitle

\RestyleAlgo{ruled}

\input{guideline-concordance}

\begin{Schunk}
\begin{Sinput}
> library(dplyr)
> library(tidyr)
> library(modelr)
> knitr::opts_chunk$set(echo = TRUE)
\end{Sinput}
\end{Schunk}

This document walks through how to apply the rational agent framework to your visualization experiment. We provide general guidance and walk through the forecast visualization example in the paper in more detail.

\section{Identifying the decision problem}

Before we get started to applying the framework to a visualization experiment, we first need to identify the decision-making problem that forms the task in the experiment. Identify the decision problem involves identifying:

\begin{itemize}
    \item \textbf{states} (the description of reality we want help users understand with visualization, e.g. whether the temperature is above or below freezing in our weather forecast example).
    \item \textbf{data generating model} (the process that generates the stimuli, i.e. the joint distribution between the state and the visualization. In the weather forecast example, the visualization is the distribution where the temperature is drawn, and the state is freezing / not freezing).
    \item an \textbf{action space} (the report space that the experiment requires users to use, e.g. whether or not to salt the parking lot).
    \item \textbf{signals} (the visualizations you show to users for helping them make decisions).
    \item a \textbf{scoring rule} (the quality score or payoff that the experiment assigns to participants depending on their action and the state in reality, e.g. how many points you lose if you salt when it doesn't freeze or don't salt but it freezes).
\end{itemize}

For the purpose of these guidelines, we will assume an experiment that is a slight variation on the weather forecast example in the paper, where we are asking users to report two actions. The first is the binary decision on whether to salt the parking lot and the second is their belief about the possibility of freezing in discrete levels after view a visualization about daily lowest temperature. The decision-making problem can be formalized as:

\begin{itemize}
    \item states: $\theta \in \Theta = \{0=\text{not freezing}, 1=\text{freezing}\}$.
    \item data generating models: daily low temperature $t \sim N(\mu, \sigma^2)$; $\Pr[\theta = 1] = \Pr[t \leq 0]$; $\mu = 5$ fixed and $\sigma$ uniformly from $\{2, 3, 4, 5\}$.
    \item action space: $a \in A = \{0 = \text{not salt}, 1 = \text{salt}\}$; $b \in B = \{0, 0.02, 0.04, 0.06, ..., 0.96, 0.98, 1\}$, reporting the belief of freezing in discrete levels.
    \item signals: $v \in V$ showing daily low temperature $t$.
    \item scoring rule: $S_{decision}(a, \theta)$ for reporting the decision of salting, see equation 3 in the paper; $S_{belief}(b, \theta) = S_{decision}(\argmax_{a}{S_{decision}(a, b)}, \theta)$ for reporting the belief of freezing.
\end{itemize}

The following R code defines the scoring rule $S_{decision}$ and $S_{belief}$. For $S_{decision}$, we follow the payoff function in equation 3 in the paper. For $S_{belief}$, we score belief by the optimal decision on belief, i.e. assuming the agent makes the decision rationally on their belief, what would be their score?

\begin{Schunk}
\begin{Sinput}
> belief_space = seq(0, 1, 0.02)
> decision_space = c(0, 1)
> decision_scoring_rule = function(decision, state) {
+   # See equation 3 in the paper
+   -100 * (1 - decision) * state - 10 * decision * (1 - state)
+ }
> belief_scoring_rule = function(belief, state) {
+   # apply the optimal decision based on the belief
+   decision = decision_space[apply(matrix(unlist(lapply(decision_space, function(d) {
+       decision_scoring_rule(d, belief)
+   })), ncol = length(belief)), 2, which.max)]
+   
+   decision_scoring_rule(decision, state)
+ }
\end{Sinput}
\end{Schunk}

\section{For the purposes of this walkthrough only: Simulate behavioral data using quantal response equilibrium}

For the purposes of our walkthrough of the rational agent approach, we generate mock experimental data for the weather forecast example described above using quantal response equilibrium\footnote{https://www.sciencedirect.com/science/article/pii/S0899825685710238} (a type of behavioral model that assumes the probability of any particular action being chosen is positively related to the payoff from that action, i.e. very bad actions are unlikely), where we keep the distribution of sigma, i.e. the distribution of data generating models, as uniform distribution.

\begin{Schunk}
\begin{Sinput}
> p_d = c(0.25, 0.25, 0.25, 0.25)
> # p(d) the prior knowledge of the distribution of data generating models
> 
> sigma_choices = c(2, 3, 4, 5)
> mu = 5
> data = data.frame(
+   mu = mu, 
+   sigma = sigma_choices,
+   vis = c("mean", "mean+interval", "gradient", "HOPs")
+ ) %>% 
+   data_grid(mu, sigma, vis) %>% 
+   rowwise() %>%
+   mutate(
+     freezing_prob = pnorm(0, mu, sigma),
+     behavioral_decision = list(sample(decision_space, 100, 
+       replace = TRUE, 
+       # quantal response equilibrium
+       prob = exp(0.2 * decision_scoring_rule(decision_space, freezing_prob)))),
+     behavioral_belief = list(sample(belief_space, 100, 
+       replace = TRUE, 
+       # quantal response equilibrium
+       prob = exp(belief_scoring_rule(belief_space, freezing_prob))))
+   ) %>%
+   unnest(cols = c(behavioral_decision, behavioral_belief))
> head(data)
\end{Sinput}
\begin{Soutput}
# A tibble: 6 × 6
     mu sigma vis      freezing_prob behavioral_decision behavioral_belief
  <dbl> <dbl> <chr>            <dbl>               <dbl>             <dbl>
1     5     2 gradient       0.00621                   0              0.48
2     5     2 gradient       0.00621                   0              0.46
3     5     2 gradient       0.00621                   0              0.2 
4     5     2 gradient       0.00621                   0              0.1 
5     5     2 gradient       0.00621                   0              0.14
6     5     2 gradient       0.00621                   0              0.06
\end{Soutput}
\end{Schunk}

\section{Pre-experiment analysis}

In this section, we will show how to calculate the three pre-experiment quantities (rational baseline, rational benchmark, and value of information) that the rational agent framework offers. The rational agent's prior belief is before seeing the visualization, the unconditional probability of each state is drawn according to the data generating model. the rational agent will update their belief on data generating models by the implied distribution of them in visualizations. For the purpose of walking through the steps entailed in applying the framework, we provide general pseudo code first, then walk through the specific example about weather forecast with full code that assumes as behavioral data is the simulated data set generated above.

\subsection{Explicit Calculations for Weather Forecasting Experiment}


\begin{description}
    \item[Prior] We have the prior probability of freezing 
\begin{align*}
    p &= \sum\nolimits_\sigma \Pr[\state = 1 , \sigma]=0.00155+0.01195+0.0264+0.0397\\
    &=0.0796.
\end{align*}

\item [Posterior] The posterior probabilities are $\Pr[\state=1|\sigma]=0.62\%, 4.78\%, 10.56\%, 15.87\%$, relatively for $\sigma=2, 3, 4, 5$. Here we take $\sigma = 2$ as an example:
\begin{equation*}
    \Pr[\state=1 | \sigma = 2] = \Pr_{ x\sim N(5, 2^2)}[x\leq 0]=0.0062.
\end{equation*}

\end{description}

The rational agent framework gives the following quantities:
\begin{description}
\item[rational baseline:] $\rprior = -7.96$.  

  The prior $\prior = 0.08$ is optimized at no-salt and gives an expected payoff of $-7.96$. The calculation is as follows:
\begin{align*}
    \rprior &= \Pr[\state=0]\cdot\score(\action = 0, \state=0) + \Pr[\state=1]\cdot\score(\action = 0, \state=1)\\
    &= (1-0.0796)\times 0 + 0.0796\times (-100)=-7.96
\end{align*}
  

\item[visualization optimal:] 
  $\rpos^{\text{CI}}=\rpos^{\text{gradient}}=\rpos^{\text{HOPs}}=-5.69$; $\rpos^{\text{mean}}=-7.96$.
  
  In CI, gradient, and HOPs, each signal arises with probability $1/4$ and the average of the optimal actions under the induced posteriors gives $\rpos = -5.69$.  For the visualization of the mean, the rational agent has only the prior information and obtains $\rpos^{\text{mean}} = \rprior = -7.59$.

  The calculation of $\rpos$ for CI, gradient, and HOPs is the following:
  \begin{align*}
      \rpos^{\text{CI}} &= \sum\nolimits_{\sigma, \state}\Pr[\sigma, \state]\cdot \score(\mathbf{1}_{\sigma\in\{4, 5\}}, \state)\\
      &= 0.24845\times 0 + 0.00155\times (-100)+0.23805\times 0 \\
      &\quad+ 0.01195\times (-100) + 0.2236\times (-10) + 0.0264\times 0\\
      &\quad + 0.2103\times (-10) + 0.0397\times 0\\
      &= -5.69
  \end{align*}
 \item[rational benchmark:] $\rperfect = \max_{\text{vis}} \rscore^{\text{vis}} = -5.69$, the best achievable across visualizations. 
\item[value of information:]  $\infoval = \rperfect - \rprior = 2.27$.%$\infoval^{\text{CI}}=\infoval^{\text{gradient}} = \infoval^{\text{HOPs}} = 2.27$; $\infoval^{\text{mean}} = 0$

  \end{description}

\subsubsection{Rational baseline}

To calculate the rational baseline, we first calculate the prior belief of rational agent and their associated action choice given these beliefs. Then we calculate what score the rational agent would get under the scoring rule if they complete the experiment with only prior knowledge.

\paragraph{Pseudo code} 
See algorithm \ref{alg:baseline}.

\begin{algorithm}[!h]
\caption{Rational baseline}\label{alg:baseline}
\KwIn{the experimental data $D$ with each row representing one experimental trial, the distribution of data generating models $p(d)$, the action space $A$, and the scoring rule $S$}
\KwOut{the rational baseline $baseline$}
$belief \gets E_{d\sim p(d)}(d)$\;\Comment{the prior belief of the data generating models}
$action \gets \argmax_{a \sim A}{E_{\theta \sim belief}(S(a, \theta))}$\;\Comment{the action made on prior belief}
$payoff \gets 0$\;
\For{$row \in D$}{
  $dist \gets \text{data generating model used in } row$\;
  $payoff \gets payoff + E_{\theta \sim dist}(S(action, \theta))$\;
}
$baseline \gets payoff / \text{\# of row in }D$\;\Comment{rational baseline is the rational agent's expected payoff in the experiment}
\end{algorithm}

\paragraph{R in weather forecast example}

\begin{Schunk}
\begin{Sinput}
> prior_belief = Reduce("+", 
+   sapply(sigma_choices, function(m) {pnorm(0, mu, m)})) / length(sigma_choices)
> decision_payoffs = lapply(decision_space, function(a) {
+   decision_scoring_rule(a, prior_belief)
+ })
> prior_reporting_decision = decision_space[[which.max(decision_payoffs)]]
> prior_reporting_decision
\end{Sinput}
\begin{Soutput}
[1] 0
\end{Soutput}
\begin{Sinput}
> belief_payoffs = lapply(belief_space, function(b) {
+   belief_scoring_rule(b, prior_belief)
+ })
> prior_reporting_belief = belief_space[[which.max(belief_payoffs)]]
> prior_reporting_belief
\end{Sinput}
\begin{Soutput}
[1] 0
\end{Soutput}
\begin{Sinput}
> # simulate the action of rational agent with only prior knowledge on all experiments
> for (i in 1:nrow(data)) {
+   data[i, "prior_decision_payoff"] = 
+     decision_scoring_rule(prior_reporting_decision, data[i, "freezing_prob"])
+   data[i, "prior_belief_payoff"] = 
+     belief_scoring_rule(prior_reporting_belief, data[i, "freezing_prob"])
+ }
> prior_payoff = data %>%
+   dplyr::group_by(vis) %>%
+   summarise(prior_decision_payoff = mean(prior_decision_payoff),
+             prior_belief_payoff = mean(prior_belief_payoff))
> prior_payoff
\end{Sinput}
\begin{Soutput}
# A tibble: 4 × 3
  vis           prior_decision_payoff prior_belief_payoff
  <chr>                         <dbl>               <dbl>
1 gradient                      -7.96               -7.96
2 HOPs                          -7.96               -7.96
3 mean                          -7.96               -7.96
4 mean+interval                 -7.96               -7.96
\end{Soutput}
\end{Schunk}

The result says the optimal expected payoff of the rational agent with only prior knowledge is $-7.96$ for both reporting decision and belief, and the optimal decision and belief are both $0$, meaning that the rational agent with only prior belief would always report $0$ and the rational baseline is $-7.96$.

\subsection{Rational benchmark}

Then we simulate the rational agent on the experimental data. After viewing the visualization signals, the rational agent will update the belief about states based on their belief about the distribution about data generating model. For example, the visualization signals $vis$ provides an information about the possibility that this visualization could occur in data generating models $d$ as $p(vis|d)$ and the prior belief about the distribution of data generating models is $p(d)$. Then the posterior belief about the distribution of data generating models could be $p(d|vis) = p(vis|d) \cdot p(d) / p(vis)$.

\paragraph{Pseudo code}
See algorithm \ref{alg:benchmark}.

\begin{algorithm}[!h]
\caption{Rational benchmark}\label{alg:benchmark}
\KwIn{the experimental data $D$ with each row representing one experimental trial, the distribution of data generating models $p(d)$, the action space $A$, and the scoring rule $S$}
\KwOut{the rational benchmark $benchmark$}
$payoff \gets 0$\;
\For{$row \in D$}{
  $vis \gets \text{visualization used in } row$\; 
  $p(d|vis) = p(vis|d) \cdot p(d) / p(vis)$\;\Comment{the posterior belief of the distribution of data generating models}
  $belief \gets E_{d\sim p(d|vis)}(d)$\;\Comment{the belief of the data generating models}
  $action \gets \argmax_{a \sim A}{E_{\theta \sim belief}(S(a, \theta))}$\;\Comment{the action made on posterior belief}
  $dist \gets \text{data generating model used in } row$\;
  $payoff \gets payoff + E_{\theta \sim dist}(S(action, \theta))$\;\Comment{the expected payoff with posterior information}
}
$benchmark \gets payoff / \text{\# of row in }D$\;
\end{algorithm}

\paragraph{R in weather forecast example}
In the weather forecast example, the visualizations provide perfect information about the variance of data except "mean" visualization, so the updated distribution of data generating models could converge to a situation that only one data generating model has possibility with almost 1 and others are all almost 0. In this case, we assume that the rational agent would know which data generating model is using with certainty after viewing one visualization. Formally, assuming that $vis$ provides perfect information about data generating model $d_1$, then

\begin{align*}
    p(vis|d_1) &\approx 1\\
    p(vis|d_2) &\approx 0\\
    ...&\\
    p(vis|d_n) &\approx 0
\end{align*}

so $p(d_1|vis) = p(vis|d) \cdot p(d) / p(vis) \approx 1$ when the visualization is fixed.

\begin{Schunk}
\begin{Sinput}
> for (i in 1:nrow(data)) {
+   # rational agent can infer the exact data generating model based on signal
+   if (data[[i, "vis"]] == "mean") {
+     posterior_belief = prior_belief
+   } else {
+     posterior_belief = data[[i, "freezing_prob"]]
+   }
+   posterior_reporting_decision = decision_space[[
+     which.max(lapply(decision_space, function(a) {
+       decision_scoring_rule(a, posterior_belief)
+     }))
+   ]]
+   posterior_reporting_belief = belief_space[[
+     which.max(lapply(belief_space, function(b) {
+       belief_scoring_rule(b, posterior_belief)
+     }))
+   ]]
+   data[[i, "posterior_decision_payoff"]] = 
+     decision_scoring_rule(posterior_reporting_decision, data[[i, "freezing_prob"]])
+   
+   data[[i, "posterior_belief_payoff"]] = 
+     belief_scoring_rule(posterior_reporting_belief, data[[i, "freezing_prob"]])
+ }
> posterior_payoff = data %>%
+   dplyr::group_by(vis) %>%
+   summarise(posterior_decision_payoff = mean(posterior_decision_payoff),
+             posterior_belief_payoff = mean(posterior_belief_payoff))
> posterior_payoff
\end{Sinput}
\begin{Soutput}
# A tibble: 4 × 3
  vis           posterior_decision_payoff posterior_belief_payoff
  <chr>                             <dbl>                   <dbl>
1 gradient                          -5.69                   -5.69
2 HOPs                              -5.69                   -5.69
3 mean                              -7.96                   -7.96
4 mean+interval                     -5.69                   -5.69
\end{Soutput}
\end{Schunk}

When seeing "mean" visualization, the rational agent didn't get the information about sigma, so they can't update the belief on data generating models, and the rational benchmark for both reporting action and belief turns out to be $-7.96$, which is the same as rational baseline. For the other visualization, the rational benchmark for both reporting action and belief is $-5.69$.

\subsection{Value of information}

We define the value of information in rational agent framework as the difference between rational benchmark and baseline, i.e. the maximum improvement can be made when agent has the right prior knowledge and follows the Bayesian rule to update the belief.

In the example of weather forecast, the value of information is \texttt{posterior\_payoff - prior\_payoff = 2.27} expect \texttt{0} for "mean" visualization.

\section{Post-experiment analysis}

In this section, we will calculate the score of behavioral agents and the score of the calibrated behavioral.

\subsection{Behavioral score}

We use the scoring rule on behavioral agents' actions.

\paragraph{Pseudo code}
See algorithm \ref{alg:behavioral}.

\begin{algorithm}[!h]
\caption{Behavioral score}\label{alg:behavioral}
\KwIn{the experimental data $D$ with each row representing one experimental trial, and the scoring rule $S$}
\KwOut{the behavioral scores $behavioral$}
$payoff \gets 0$\;
\For{$row \in D$}{
  $dist \gets \text{data generating model used in } row$\;
  $action \gets \text{action made by the behavioral agent in } row$\;
  $payoff \gets payoff + E_{\theta \sim dist}(S(action, \theta))$\;\Comment{the expected score of behavioral action}
}
$behavioral \gets payoff / \text{\# of row in }D$\;
\end{algorithm}

\paragraph{R in weather forecast example}

\begin{Schunk}
\begin{Sinput}
> for (i in 1:nrow(data)) {
+   data[i, "behavioral_decision_payoff"] = 
+     decision_scoring_rule(data[i, "behavioral_decision"], data[i, "freezing_prob"])
+   data[i, "behavioral_belief_payoff"] = 
+     belief_scoring_rule(data[i, "behavioral_belief"], data[i, "freezing_prob"])
+ }
> behavioral_payoff = data %>%
+   dplyr::group_by(vis) %>%
+   summarise(behavioral_decision_payoff = mean(behavioral_decision_payoff),
+             behavioral_belief_payoff = mean(behavioral_belief_payoff))
> behavioral_payoff
\end{Sinput}
\begin{Soutput}
# A tibble: 4 × 3
  vis           behavioral_decision_payoff behavioral_belief_payoff
  <chr>                              <dbl>                    <dbl>
1 gradient                           -6.91                    -8.52
2 HOPs                               -6.85                    -8.52
3 mean                               -6.73                    -8.51
4 mean+interval                      -7.01                    -8.46
\end{Soutput}
\end{Schunk}

Since the behavioral agent's actions are generated randomly but not from real persons, some of the payoffs are below rational baseline, e.g. the behavioral payoff when reporting belief. Then we will introduce the calibrated behavioral score, which can represent the behavioral in the baseline-and-benchmark scale.

\subsection{Calibrated behavioral score}

The rational baseline and the rational benchmark could be considered as the evaluating scale for behavioral agents when assuming that the agent gets the right prior knowledge, updates the belief by Bayesian rule, and optimizes the decision based on belief. However, unfortunately, behavioral agents may not follow these requirements, which makes them less comparable to the scale. Thus, we propose the calibrated behavioral score, where the rational agent calibrates the behavioral agent's behaviors into Bayesian and thus falls into the interval between rational baseline and benchmark.

In the calibration, the rational agent is not using the visualizations as signals but using the actions made by behavioral agents as signals. When getting a behavioral agent's action $a$, they will look at what's the possibilities that this action will appear in different data generating models ($p(a|d)$) and use these possibilities to update the prior belief ($p(d|a) = p(a|d) \cdot p(d) / p(a)$).

The pseudo code is as following...

\paragraph{Pseudo code}
See algorithm \ref{alg:calibrated}.

\begin{algorithm}[!h]
\caption{Calibrated behavioral score}\label{alg:calibrated}
\KwIn{the experimental data $D$ with each row representing one experimental trial, the distribution of data generating models $p(d)$, the action space $A$, and the scoring rule $S$}
\KwOut{the calibrated behavioral score $benchmark$}
$payoff \gets 0$\;
\For{$row \in D$}{
  $signal \gets \text{action made by the behavioral agent in } row$
  $p(d|signal) = p(signal|d) \cdot p(d) / p(signal)$\; \Comment{the posterior belief of the distribution of data generating models in calibration}
  $belief \gets E_{d\sim p(d|signal)}(d)$\;\Comment{the belief of the data generating models}
  $action \gets \argmax_{a \sim A}{E_{\theta \sim belief}(S(a, \theta))}$\; \Comment{the action made on posterior belief}
  $dist \gets \text{data generating model used in } row$\;
  $payoff \gets payoff + E_{\theta \sim dist}(S(action, \theta))$\; \Comment{the expected payoff with posterior information}
}
$benchmark \gets payoff / \text{\# of row in }D$\;
\end{algorithm}

\paragraph{R in weather forecast example}

\begin{Schunk}
\begin{Sinput}
> # the joint distribution of actions and data generating models
> joint_decision_distribution = data %>%
+   dplyr::group_by(mu, sigma, freezing_prob, behavioral_decision) %>%
+   summarise(count = n()) %>%
+   dplyr::group_by(mu, sigma, freezing_prob) %>%
+   mutate(count = count / sum(count))
> joint_belief_distribution = data %>%
+   dplyr::group_by(mu, sigma, freezing_prob, behavioral_belief) %>%
+   summarise(count = n()) %>%
+   dplyr::group_by(mu, sigma, freezing_prob) %>%
+   mutate(count = count / sum(count))
> sum_payoff = 0
> for (i in 1:nrow(data)) {
+   decision_signal = data[[i, "behavioral_decision"]]
+   
+   joint_decision_distribution_filtered = joint_decision_distribution %>%
+     filter(behavioral_decision == decision_signal)
+   
+   distributions = pnorm(0, mu, sigma_choices)
+   
+   p_a = nrow(data %>% filter(behavioral_decision == decision_signal)) / nrow(data)
+   
+   sigmas = joint_decision_distribution_filtered$sigma
+   p_a_d = as.list(rep(0, length(sigma_choices)))
+   names(p_a_d) = as.character(sigma_choices)
+   p_a_d[as.character(sigmas)] = joint_decision_distribution_filtered$count  # p(a|d)
+   p_a_d = unname(unlist(p_a_d))
+   p_d_a = p_a_d * p_d / p_a  # p(a|d) * p(d) / p(a)
+   
+   calibrated_belief = sum(distributions * p_d_a)  # E_p_d_a(d)
+   calibrated_reporting_decision = decision_space[[which.max(
+     lapply(decision_space, function(a) {
+       decision_scoring_rule(a, calibrated_belief)
+     }))]]
+   data[[i, "calibrated_decision_payoff"]] = 
+     decision_scoring_rule(calibrated_reporting_decision, data[[i, "freezing_prob"]])
+   
+   
+   belief_signal = data[[i, "behavioral_belief"]]
+   
+   joint_belief_distribution_filtered = joint_belief_distribution %>%
+     filter(behavioral_belief == belief_signal)
+   
+   p_a = nrow(data %>% filter(behavioral_belief == belief_signal)) / nrow(data)
+   
+   sigmas = joint_belief_distribution_filtered$sigma
+   p_a_d = as.list(rep(0, length(sigma_choices)))
+   names(p_a_d) = as.character(sigma_choices)
+   p_a_d[as.character(sigmas)] = joint_belief_distribution_filtered$count  # p(a|d)
+   p_a_d = unname(unlist(p_a_d))
+   p_d_a = p_a_d * p_d / p_a  # p(a|d) * p(d) / p(a)
+   
+   calibrated_belief = sum(distributions * p_d_a)  # E_p_d_a(d)
+   calibrated_reporting_belief = belief_space[[which.max(lapply(belief_space, function(a) {
+     belief_scoring_rule(a, calibrated_belief)
+   }))]]
+   data[[i, "calibrated_belief_payoff"]] = 
+     belief_scoring_rule(calibrated_reporting_belief, data[[i, "freezing_prob"]])
+ }
> calibrated_payoff = data %>%
+   dplyr::group_by(vis) %>%
+   summarise(calibrated_decision_payoff = mean(calibrated_decision_payoff),
+             calibrated_belief_payoff = mean(calibrated_belief_payoff))
> calibrated_payoff
\end{Sinput}
\begin{Soutput}
# A tibble: 4 × 3
  vis           calibrated_decision_payoff calibrated_belief_payoff
  <chr>                              <dbl>                    <dbl>
1 gradient                           -6.91                    -5.76
2 HOPs                               -6.85                    -5.77
3 mean                               -6.73                    -5.74
4 mean+interval                      -7.01                    -5.74
\end{Soutput}
\end{Schunk}

Now we get the calibrated behavioral scores. The calibrated scores of all visualization types are between rational baseline and rational benchmark.

\end{document}
